{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "975cc8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "516efb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.97      0.97      0.97       100\n",
      "           B       0.97      0.98      0.98       100\n",
      "       Blank       0.98      1.00      0.99       100\n",
      "           C       0.97      0.93      0.95       100\n",
      "           D       0.89      0.91      0.90       100\n",
      "           E       0.93      0.93      0.93       100\n",
      "           F       0.79      0.92      0.85       100\n",
      "           G       0.91      0.94      0.93       100\n",
      "           H       0.96      0.96      0.96       100\n",
      "           I       0.99      0.88      0.93       100\n",
      "           J       0.94      0.94      0.94       100\n",
      "           K       0.93      0.92      0.92       100\n",
      "           L       0.90      0.90      0.90       100\n",
      "           M       0.88      0.73      0.80       100\n",
      "           N       0.87      0.90      0.89       100\n",
      "           O       0.93      0.91      0.92       100\n",
      "           P       0.60      0.89      0.72       100\n",
      "           Q       0.80      0.88      0.84       100\n",
      "           R       0.92      0.86      0.89       100\n",
      "           S       0.97      0.90      0.93       100\n",
      "           T       0.96      0.96      0.96       100\n",
      "           U       0.98      0.87      0.92       100\n",
      "           V       0.89      0.90      0.90       100\n",
      "           W       0.93      0.83      0.88       100\n",
      "           X       0.99      0.80      0.88       100\n",
      "           Y       0.99      0.90      0.94       100\n",
      "           Z       0.85      0.96      0.90       100\n",
      "\n",
      "    accuracy                           0.91      2700\n",
      "   macro avg       0.91      0.91      0.91      2700\n",
      "weighted avg       0.91      0.91      0.91      2700\n",
      "\n",
      "Classification Report as DataFrame:\n",
      "    Class  Precision  Recall  F1-Score  Support\n",
      "0       A       0.97    0.97      0.97      100\n",
      "1       B       0.97    0.98      0.98      100\n",
      "2   Blank       0.98    1.00      0.99      100\n",
      "3       C       0.97    0.93      0.95      100\n",
      "4       D       0.89    0.91      0.90      100\n",
      "5       E       0.93    0.93      0.93      100\n",
      "6       F       0.79    0.92      0.85      100\n",
      "7       G       0.91    0.94      0.93      100\n",
      "8       H       0.96    0.96      0.96      100\n",
      "9       I       0.99    0.88      0.93      100\n",
      "10      J       0.94    0.94      0.94      100\n",
      "11      K       0.93    0.92      0.92      100\n",
      "12      L       0.90    0.90      0.90      100\n",
      "13      M       0.88    0.73      0.80      100\n",
      "14      N       0.87    0.90      0.89      100\n",
      "15      O       0.93    0.91      0.92      100\n",
      "16      P       0.60    0.89      0.72      100\n",
      "17      Q       0.80    0.88      0.84      100\n",
      "18      R       0.92    0.86      0.89      100\n",
      "19      S       0.97    0.90      0.93      100\n",
      "20      T       0.96    0.96      0.96      100\n",
      "21      U       0.98    0.87      0.92      100\n",
      "22      V       0.89    0.90      0.90      100\n",
      "23      W       0.93    0.83      0.88      100\n",
      "24      X       0.99    0.80      0.88      100\n",
      "25      Y       0.99    0.90      0.94      100\n",
      "26      Z       0.85    0.96      0.90      100\n",
      "\n",
      "=== Summary Statistics ===\n",
      "Average Precision: 0.9144\n",
      "Average Recall: 0.9063\n",
      "Average F1-Score: 0.9081\n",
      "Total Support: 2700\n",
      "\n",
      "=== Best Performing Classes (by F1-Score) ===\n",
      "    Class  F1-Score\n",
      "2   Blank      0.99\n",
      "1       B      0.98\n",
      "0       A      0.97\n",
      "8       H      0.96\n",
      "20      T      0.96\n",
      "\n",
      "=== Worst Performing Classes (by F1-Score) ===\n",
      "   Class  F1-Score\n",
      "16     P      0.72\n",
      "13     M      0.80\n",
      "17     Q      0.84\n",
      "6      F      0.85\n",
      "23     W      0.88\n"
     ]
    }
   ],
   "source": [
    "# Read the classification report from file\n",
    "with open('Data 1 Classification', 'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)\n",
    "\n",
    "# Alternatively, parse it into a DataFrame for easier analysis\n",
    "def parse_classification_report(file_path):\n",
    "    \"\"\"Parse classification report into a pandas DataFrame\"\"\"\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Extract data lines (skip header and footer)\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        if line.strip() and not line.startswith('Classification'):\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 4 and parts[0] not in ['accuracy', 'macro', 'weighted', 'precision']:\n",
    "                try:\n",
    "                    # Regular class rows - try to convert to ensure it's data\n",
    "                    label = parts[0]\n",
    "                    precision = float(parts[1])\n",
    "                    recall = float(parts[2])\n",
    "                    f1_score = float(parts[3])\n",
    "                    support = int(parts[4])\n",
    "                    data.append([label, precision, recall, f1_score, support])\n",
    "                except ValueError:\n",
    "                    # Skip header lines or any non-numeric rows\n",
    "                    continue\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, columns=['Class', 'Precision', 'Recall', 'F1-Score', 'Support'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Parse the report\n",
    "df = parse_classification_report('Data 1 Classification')\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nClassification Report as DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Show basic statistics\n",
    "print(\"\\n=== Summary Statistics ===\")\n",
    "print(f\"Average Precision: {df['Precision'].mean():.4f}\")\n",
    "print(f\"Average Recall: {df['Recall'].mean():.4f}\")\n",
    "print(f\"Average F1-Score: {df['F1-Score'].mean():.4f}\")\n",
    "print(f\"Total Support: {df['Support'].sum()}\")\n",
    "\n",
    "# Identify best and worst performing classes\n",
    "print(\"\\n=== Best Performing Classes (by F1-Score) ===\")\n",
    "print(df.nlargest(5, 'F1-Score')[['Class', 'F1-Score']])\n",
    "print(\"\\n=== Worst Performing Classes (by F1-Score) ===\")\n",
    "print(df.nsmallest(5, 'F1-Score')[['Class', 'F1-Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6d794f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'r2_score' from 'StatsFunctions' (c:\\Users\\oneal\\OneDrive\\Desktop\\Stats\\ASL-project\\StatsFunctions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mStatsFunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     r2_score, \n\u001b[32m      3\u001b[39m     mean_squared_error, \n\u001b[32m      4\u001b[39m     mean_absolute_error, \n\u001b[32m      5\u001b[39m     linear_regression_fit, \n\u001b[32m      6\u001b[39m     linear_regression_predict, \n\u001b[32m      7\u001b[39m     regression_analysis\n\u001b[32m      8\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'r2_score' from 'StatsFunctions' (c:\\Users\\oneal\\OneDrive\\Desktop\\Stats\\ASL-project\\StatsFunctions.py)"
     ]
    }
   ],
   "source": [
    "from StatsFunctions import (\n",
    "    r2_score, \n",
    "    mean_squared_error, \n",
    "    mean_absolute_error, \n",
    "    linear_regression_fit, \n",
    "    linear_regression_predict, \n",
    "    regression_analysis\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f80d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the classification report\n",
    "def parse_classification_report(file_path):\n",
    "    \"\"\"Parse classification report into a pandas DataFrame\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    data = []\n",
    "    for line in lines:\n",
    "        if line.strip() and not line.startswith('Classification'):\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 4 and parts[0] not in ['accuracy', 'macro', 'weighted', 'precision']:\n",
    "                try:\n",
    "                    label = parts[0]\n",
    "                    precision = float(parts[1])\n",
    "                    recall = float(parts[2])\n",
    "                    f1_score = float(parts[3])\n",
    "                    support = int(parts[4])\n",
    "                    data.append([label, precision, recall, f1_score, support])\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=['Class', 'Precision', 'Recall', 'F1-Score', 'Support'])\n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading classification report...\")\n",
    "df = parse_classification_report('Data 1 Classification')\n",
    "print(f\"Loaded {len(df)} classes\\n\")\n",
    "print(df.head(10))\n",
    "print()\n",
    "\n",
    "# Create index as predictor (class position)\n",
    "X = np.arange(len(df)).astype(float)\n",
    "\n",
    "# Perform regression for each metric\n",
    "metrics_results = {}\n",
    "models = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMING REGRESSION ANALYSIS ON EACH METRIC\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# 1. Precision Regression\n",
    "print(\"1. PRECISION\")\n",
    "print(\"-\" * 50)\n",
    "slope_p, intercept_p = linear_regression_fit(X, df['Precision'].values)\n",
    "precision_pred = linear_regression_predict(X, slope_p, intercept_p)\n",
    "models['Precision'] = (slope_p, intercept_p)\n",
    "metrics_results['Precision'] = regression_analysis(df['Precision'].values, precision_pred, \"Precision\")\n",
    "print(f\"Regression equation: Precision = {slope_p:.6f} * Class_Index + {intercept_p:.6f}\\n\")\n",
    "\n",
    "# 2. Recall Regression\n",
    "print(\"2. RECALL\")\n",
    "print(\"-\" * 50)\n",
    "slope_r, intercept_r = linear_regression_fit(X, df['Recall'].values)\n",
    "recall_pred = linear_regression_predict(X, slope_r, intercept_r)\n",
    "models['Recall'] = (slope_r, intercept_r)\n",
    "metrics_results['Recall'] = regression_analysis(df['Recall'].values, recall_pred, \"Recall\")\n",
    "print(f\"Regression equation: Recall = {slope_r:.6f} * Class_Index + {intercept_r:.6f}\\n\")\n",
    "\n",
    "# 3. F1-Score Regression\n",
    "print(\"3. F1-SCORE\")\n",
    "print(\"-\" * 50)\n",
    "slope_f, intercept_f = linear_regression_fit(X, df['F1-Score'].values)\n",
    "f1_pred = linear_regression_predict(X, slope_f, intercept_f)\n",
    "models['F1-Score'] = (slope_f, intercept_f)\n",
    "metrics_results['F1-Score'] = regression_analysis(df['F1-Score'].values, f1_pred, \"F1-Score\")\n",
    "print(f\"Regression equation: F1-Score = {slope_f:.6f} * Class_Index + {intercept_f:.6f}\\n\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 14))\n",
    "fig.suptitle('Regression Analysis: Classification Metrics', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "predictions = [precision_pred, recall_pred, f1_pred]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "for idx, (metric, pred, color) in enumerate(zip(metrics, predictions, colors)):\n",
    "    actual = df[metric].values\n",
    "    residuals = actual - pred\n",
    "    \n",
    "    # Column 1: Actual vs Predicted with trend line\n",
    "    axes[idx, 0].scatter(X, actual, alpha=0.7, s=80, label='Actual', color=color, edgecolors='black', linewidth=0.5)\n",
    "    axes[idx, 0].plot(X, pred, 'r--', lw=2, label='Regression Line')\n",
    "    axes[idx, 0].set_xlabel('Class Index', fontsize=10)\n",
    "    axes[idx, 0].set_ylabel(metric, fontsize=10)\n",
    "    axes[idx, 0].set_title(f'{metric}: Actual vs Trend', fontsize=11, fontweight='bold')\n",
    "    axes[idx, 0].legend()\n",
    "    axes[idx, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Column 2: Actual vs Predicted scatter\n",
    "    axes[idx, 1].scatter(actual, pred, alpha=0.7, s=80, color=color, edgecolors='black', linewidth=0.5)\n",
    "    axes[idx, 1].plot([actual.min(), actual.max()], \n",
    "                       [actual.min(), actual.max()], \n",
    "                       'r--', lw=2, label='Perfect Fit')\n",
    "    axes[idx, 1].set_xlabel(f'Actual {metric}', fontsize=10)\n",
    "    axes[idx, 1].set_ylabel(f'Predicted {metric}', fontsize=10)\n",
    "    axes[idx, 1].set_title(f'{metric}: Prediction Accuracy', fontsize=11, fontweight='bold')\n",
    "    axes[idx, 1].legend()\n",
    "    axes[idx, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Column 3: Residuals\n",
    "    axes[idx, 2].scatter(pred, residuals, alpha=0.7, s=80, color=color, edgecolors='black', linewidth=0.5)\n",
    "    axes[idx, 2].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "    axes[idx, 2].set_xlabel(f'Predicted {metric}', fontsize=10)\n",
    "    axes[idx, 2].set_ylabel('Residuals', fontsize=10)\n",
    "    axes[idx, 2].set_title(f'{metric}: Residual Plot', fontsize=11, fontweight='bold')\n",
    "    axes[idx, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY: R² COMPARISON ACROSS METRICS\")\n",
    "print(\"=\"*70)\n",
    "summary_data = []\n",
    "for metric in metrics:\n",
    "    summary_data.append({\n",
    "        'Metric': metric,\n",
    "        'R²': metrics_results[metric]['r2'],\n",
    "        'RMSE': metrics_results[metric]['rmse'],\n",
    "        'MAE': metrics_results[metric]['mae']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Additional insights\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INSIGHTS\")\n",
    "print(\"=\"*70)\n",
    "for metric in metrics:\n",
    "    slope, intercept = models[metric]\n",
    "    if slope > 0:\n",
    "        trend = \"INCREASING\"\n",
    "    elif slope < 0:\n",
    "        trend = \"DECREASING\"\n",
    "    else:\n",
    "        trend = \"FLAT\"\n",
    "    print(f\"{metric}: {trend} trend (slope = {slope:.6f})\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
